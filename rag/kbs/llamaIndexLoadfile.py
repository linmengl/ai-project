from llama_index.llms.ollama import Ollama
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import gradio as gr
import os

# ç¦ç”¨ tokenizer å¹¶è¡ŒåŠ é€Ÿé¿å…å‘Šè­¦
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 1. åˆå§‹åŒ– LLM
Settings.llm = Ollama(
    model="llama3:8b",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡åŠ©æ‰‹ï¼Œè¯·å§‹ç»ˆä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­”é—®é¢˜ã€‚",
    temperature=0.7)

# 2. åŠ è½½æ–‡æ¡£ï¼šæ”¯æŒ PDFã€Wordã€Markdown
reader = SimpleDirectoryReader(
    input_dir="docs",
    required_exts=[".pdf", ".docx", ".md"]
)
documents = reader.load_data()

# embedding
embedding_model = HuggingFaceEmbedding(model_name="shibing624/text2vec-base-multilingual")

# æ„å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents, embed_model=embedding_model)

# æŒä¹…åŒ–
index.storage_context.persist(persist_dir="./storage")

query_engine = index.as_query_engine()

def ask_question(query):
    retriever = index.as_retriever(similarity_top_k=20)
    nodes = retriever.retrieve(query)
    for i, node in enumerate(nodes):
        print(f"æ–‡æ¡£ç‰‡æ®µ {i + 1}:\n", node.text[:800])
    response = query_engine.query(query)
    return str(response)

gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(label="è¯·è¾“å…¥ä½ çš„é—®é¢˜"),
    outputs=gr.Textbox(label="å›ç­”"),
    title="ğŸ“š æ–‡æ¡£é—®ç­”ç³»ç»Ÿ"
).launch(share=True)

